{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e50a66e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Anthony\\Documents\\ragggg\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45b00c0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name          0\n",
      "Region        0\n",
      "SumUp         0\n",
      "Biography     0\n",
      "Story        31\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "##searching if their is missing data in the csv\n",
    "##missing 31 stories because riot still doesn't wrote it \n",
    "df = pd.read_csv(\"../data/lore.csv\")\n",
    "print(df.isnull().sum())\n",
    "null_data = df[df.isnull().any(axis=1)]\n",
    "df.fillna(\"L'éditeur Riot Games n'a pas encore fourni l'histoire de ce champion\", inplace = True)\n",
    "df\n",
    "#null_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a4b01a95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 384)\n"
     ]
    }
   ],
   "source": [
    "sentences = [\"This is an example sentence\", \"Each sentence is converted\"]\n",
    "\n",
    "model = SentenceTransformer('sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2')\n",
    "embeddings = model.encode(sentences)\n",
    "\n",
    "print(embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5211922c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        Name     Region                                              SumUp  \\\n",
      "0     AATROX  RUNETERRA  Autrefois, Aatrox et ses frères étaient honoré...   \n",
      "1       AHRI      IONIA  Connectée à la magie du royaume spirituel, Ahr...   \n",
      "2      AKALI      IONIA  Ayant abandonné l'Ordre Kinkou et le titre de ...   \n",
      "3     AKSHAN    SHURIMA  Se jouant du danger, Akshan combat le mal sans...   \n",
      "4    ALISTAR  RUNETERRA  Alistar est un guerrier redoutable cherchant à...   \n",
      "..       ...        ...                                                ...   \n",
      "166     ZERI       ZAUN  Zeri est une jeune femme vive et téméraire ori...   \n",
      "167    ZIGGS       ZAUN  Amoureux des grosses bombes et des mèches cour...   \n",
      "168   ZILEAN  RUNETERRA  Autrefois membre du conseil d'Icathia, Zilean ...   \n",
      "169      ZOÉ     TARGON  Incarnation de l'espièglerie, de l'imagination...   \n",
      "170     ZYRA      IXTAL  Née au cours d'une ancienne catastrophe magiqu...   \n",
      "\n",
      "                                             Biography  \\\n",
      "0    Qu'on la prenne pour un dieu ou pour un démon,...   \n",
      "1    Pendant le plus clair de sa vie, Ahri n'a rien...   \n",
      "2    Ionia a toujours été une terre de magie sauvag...   \n",
      "3    Agissant dans l'ombre, un preux justicier traq...   \n",
      "4    Bien des civilisations ont résisté à Noxus, ma...   \n",
      "..                                                 ...   \n",
      "166  Originaire d'une famille ouvrière, Zeri a gran...   \n",
      "167  Ziggs naquit avec un talent certain pour le br...   \n",
      "168  Icathia n'a pas toujours été la plus désolée d...   \n",
      "169  Conformément à la nature de la Manifestation t...   \n",
      "170  Zyra a une très longue mémoire, aussi ancienne...   \n",
      "\n",
      "                                                 Story  \n",
      "0    Les ténèbres. Le souffle que je ne peux pas pr...  \n",
      "1    Le marché embaumait l'encens et les légumes po...  \n",
      "2    « Aïe… Hé ! Bo'lii ! Tu n'y vas pas de main mo...  \n",
      "3    Shadya n'était morte que depuis quelques semai...  \n",
      "4                                                  NaN  \n",
      "..                                                 ...  \n",
      "166  « Je ne peux pas accepter », déclara le commer...  \n",
      "167  Très cher Zaun. Je suis là, je suis pelucheux ...  \n",
      "168                                                NaN  \n",
      "169  Dès qu'elle pensa à la boutique de gâteaux, Zo...  \n",
      "170  L'humidité et l'odeur écœurante du marché de T...  \n",
      "\n",
      "[171 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('../data/lore.csv')\n",
    "print(df) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "9cc9595a",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "expected string or bytes-like object, got 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[39]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m text_story = row[\u001b[33m'\u001b[39m\u001b[33mStory\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     15\u001b[39m texts_bio = text_splitter.create_documents([text_biography],[{\u001b[33m\"\u001b[39m\u001b[33mSource\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mBiography\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mName\u001b[39m\u001b[33m\"\u001b[39m: row[\u001b[33m'\u001b[39m\u001b[33mName\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mRegion\u001b[39m\u001b[33m\"\u001b[39m: row[\u001b[33m'\u001b[39m\u001b[33mRegion\u001b[39m\u001b[33m'\u001b[39m], \u001b[33m\"\u001b[39m\u001b[33mSum_up\u001b[39m\u001b[33m\"\u001b[39m: row[\u001b[33m'\u001b[39m\u001b[33mSumUp\u001b[39m\u001b[33m'\u001b[39m]}])\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m text_story = \u001b[43mtext_splitter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcreate_documents\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mtext_story\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSource\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mStory\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mName\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mName\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mRegion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mRegion\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mSum_up\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mSumUp\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m}\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     17\u001b[39m liste_chunk.append(texts_bio)\n\u001b[32m     18\u001b[39m liste_chunk.append(text_story)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Anthony\\Documents\\ragggg\\Lib\\site-packages\\langchain_text_splitters\\base.py:86\u001b[39m, in \u001b[36mTextSplitter.create_documents\u001b[39m\u001b[34m(self, texts, metadatas)\u001b[39m\n\u001b[32m     84\u001b[39m index = \u001b[32m0\u001b[39m\n\u001b[32m     85\u001b[39m previous_chunk_len = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m chunk \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msplit_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m     87\u001b[39m     metadata = copy.deepcopy(_metadatas[i])\n\u001b[32m     88\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._add_start_index:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Anthony\\Documents\\ragggg\\Lib\\site-packages\\langchain_text_splitters\\character.py:149\u001b[39m, in \u001b[36mRecursiveCharacterTextSplitter.split_text\u001b[39m\u001b[34m(self, text)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msplit_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text: \u001b[38;5;28mstr\u001b[39m) -> \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;28mstr\u001b[39m]:\n\u001b[32m    141\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Split the input text into smaller chunks based on predefined separators.\u001b[39;00m\n\u001b[32m    142\u001b[39m \n\u001b[32m    143\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    147\u001b[39m \u001b[33;03m        List[str]: A list of text chunks obtained after splitting.\u001b[39;00m\n\u001b[32m    148\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m149\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_split_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_separators\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Anthony\\Documents\\ragggg\\Lib\\site-packages\\langchain_text_splitters\\character.py:109\u001b[39m, in \u001b[36mRecursiveCharacterTextSplitter._split_text\u001b[39m\u001b[34m(self, text, separators)\u001b[39m\n\u001b[32m    107\u001b[39m     separator = _s\n\u001b[32m    108\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mre\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_separator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    110\u001b[39m     separator = _s\n\u001b[32m    111\u001b[39m     new_separators = separators[i + \u001b[32m1\u001b[39m :]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\Python312\\Lib\\re\\__init__.py:177\u001b[39m, in \u001b[36msearch\u001b[39m\u001b[34m(pattern, string, flags)\u001b[39m\n\u001b[32m    174\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msearch\u001b[39m(pattern, string, flags=\u001b[32m0\u001b[39m):\n\u001b[32m    175\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Scan through string looking for a match to the pattern, returning\u001b[39;00m\n\u001b[32m    176\u001b[39m \u001b[33;03m    a Match object, or None if no match was found.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpattern\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mflags\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msearch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstring\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: expected string or bytes-like object, got 'float'"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=300,\n",
    "    chunk_overlap=30,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "\n",
    "liste_chunk = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    text_biography = row['Biography']\n",
    "    text_story = row['Story']\n",
    "    texts_bio = text_splitter.create_documents([text_biography],[{\"Source\": \"Biography\", \"Name\": row['Name'], \"Region\": row['Region'], \"Sum_up\": row['SumUp']}])\n",
    "    text_story = text_splitter.create_documents([text_story], [{\"Source\": \"Story\",\"Name\": row['Name'], \"Region\": row['Region'], \"Sum_up\": row['SumUp']}])\n",
    "    liste_chunk.append(texts_bio)\n",
    "    liste_chunk.append(text_story)\n",
    "\n",
    "\n",
    "liste_chunk\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ragggg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
